
In order to support nimble.trainAndApply() and other top level nimble functions which either execute functionality of an underlying package, or query details about its api and usage, we are employing object oriented programming to organize interfaces for each desired package.

Objects are particularly suited for this application. Firstly, using abstract methods, we can very precisely define tasks that need to be implemented by an interface, which will make an interface easier to implement, as well as easier to use. Secondly, using inheritance, we can share code that will be necessary for multiple, and sometimes all, interfaces. Thirdly, object attributes can be used to contain and control implementation specific details shared my multiple methods. Fourthly, by instantiating an object at runtime, we are automatically given the means to do any setup, warmup, or validation of the package backend. Lastly, objects define a precise implementation boundary, and the supportive framework in nimble ensures that a package implementor need only code within a single class definition, in a single file, in order to create a working interface.


*** UniversalInterface ***

The root of this object hierarchy is to be named UniversalInterface. It will contain both the abstract function definitions that must be implemented for a working interface and shared code that will be used by all interfaces in the form of concrete methods. The main and most challenging task that each instantiable object in this hierarchy must implement is the workflow necessary to perform a nimble.trainAndApply() call (and by extension, a nimble.train() call); which both the abstract methods and shared code will combine to perform. The trainAndApply() workflow is as follows:

1) training data and parameters passed as arguments to trainAndApply() will need to be transformed into acceptable types for the underlying package.
2) The transformed training data and parameters will be used to instantiate / train a learner.
3) This learner will then be applied to a transformed test data set, producing some kind of output (for example, predicted labels).
4) The output will then be transformed to a nimble appropriate type and returned.

The choice of organization in the above list is meant to help explain the structure and control flow of the the implementation defined by UniversalInterface. First note that items (1), (4) and the testing data input transformation in (3) are executed in concrete code in Universal interface, employing as subroutines both the abstract methods whose implementation is provided by the derived classes. Items (2) and (3) without transformation are implemented directly by abstract methods. In other words, UniversalInterface's concrete code is meant to do all of the set up for both input and output transformation. This means that the api of the abstract methods is as simple as possible. In the case of transformation, all the relevant information is provided as arguments to the abstract method. In the case of the implementations of tasks (2) and (3) all needed parameters will already be in a package appropriate format so their implementations do not need to take into account anything at the nimble or UniversalInterface level.

Furthermore, the workflow task list displays separations that can be made with respect to the control flow offered to the nimble user. The trainAndApply() function offers 1->2->3->4 all at once in a single function. However, we also offer the user a function which performs 1->2, then returns a wrapper object named TrainedLearner which (among other things) has a method named applyTo(), performing 3->4. Note that the shared code and the setup of the TrainedLearner object ensures that the two control flows are functionally identical given the same inputs, by the simple fact that they rely on the same abstract functions and helpers.

This workflow is implemented in UniversalInterface.trainAndApply() and UniversalInterface.train() (mirroring the top level nimble of the same name) depending on whether a TrainedLearner is returned after (2). Each item in the workflow list (and the TrainedLearner) will now be examined in more detail, finishing with a section on things not directly involved in the trainAndApply() workflow, and how nimble interacts with UniversalInterface.


*** (1) Input transformation for trainAndApply() ***

Input argument transformation is meant to operate as a buffer between values provided by a nimble user (which may be formated using nimble only types or syntax) and abstract function implementations which perform package-level tasks, doing so without regard to nimble idiom. Some portions of the process are implemented in the concrete code of UniversalInterface, and the rest is implemented in by the abstract function UniversalInterface._inputTransformation().

nimble.trainAndApply() allows for two kinds of shorthand in its 'arguments' parameter to allow a nimble user to state the name of an in-package construction (object, file, etc.) that they want to be made and then passed to the desired learner. UniversalInterface's portion of input transformation is to resolve that shorthand, and make use of the abstract function UniversalInterface.findCallable() to get some callable reference which will initiate whatever package level operations are necessary to produce the in-package construction. Also used is the abstract function UniversalInterface.getParameterNames() which will get the names of the arguments to be passed to the return from findCallable(). The return value of the callable reference will be used in _trainer(), so it is suggested that the package implementor ensure it is some meaningful value. But note that nimble has no stake in what happens within the called function, or the type of the return value; they are only ever used in the scope of functions left to the package implementor.

The shorthand for triggering that operation is as follows. If there is a key-value pairing in 'arguments' in which the value is a string to which findCallable() successfully gives a return, then UniversalInterface will try to call that callable return value, triggering whatever appropriate package level operations to produce the desired construction. Then the value in the 'arguments' pairing is replaced with the return value from the callable. The two variants in shorthand are about specifying which of two strategies are used to get the parameters that will be passed to the callable reference. In the first case, parameters will be assigned from those also present in 'arguments'. Alternatively, all the parameters of an in-package construction named 'foo' may be defined in a different key value pair of 'arguments', with the key 'foo' and a value containing a dictionary with the same parameter name to parameter value mapping format as 'arguments'.

After all such shorthand has been resolved, the package implementor is then given the opportunity to perform transformation on all the arguments that will eventually be passed down to the package level. The API to do so is provided by the abstract function UniversalInterface.inputTransformation(). This function takes as input the learner name as context, and then 4 arguments which correspond to inputs that may need to be transformed. The first three are named 'trainX', 'trainY', and 'testX' corresponding parameters of the same name in the trainAndApply() and train() calls, and will contain the corresponding values to be transformed. The last argument is a dictionary, which contains all other inputs needing to be transformed, using names mapped to values. In the case of trainAndApply() or train() this directly corresponds to the 'arguments' parameter (after all the in-package constructions have been triggered). If input transformation is being called in preparation for a function other than trainAndApply() or train(), the the first three arguments mentioned will be empty. For other transformation tasks, (such as wrapping the application method in (3)) other subsets of the parameters may be missing.

The package implementor may process those values however they see fit; the results will only ever be used by functions that perform package level operations. However, the setup for those calls will be implemented by UniversalInterface, so an output format is enforced for _inputTransformation. Specifically, the return value will be a dictionary which mirrors the structure of the inputs as much as possible. Four key value pairs are required to be present in the returned dictionary. Firstly, three keys with names 'trainX', 'trainY', and 'testX' will be associated with the transformed values of the input parameters of the same name. Lastly, a key 'arguments' must be associated with a dictionary, containing the same keys as the input parameter 'arguments', with values being the transformed values of those present in the input dictionary.

Though those four keys are the only ones required, the package implementor may also add other key value pairs as they see necessary. The full return value of _inputTransformation() will be an argument to the output transformation function, so the added values allows for communication of information necessary to undo input transformation. For example, if the learner is a classifier, and the package requires a certain range of values for classes, then input transformation is a excellent place to implement the remapping of classes. This remapping could be recorded as an extra key value pairing in the output, and the mapping could then be properly undone in output transformation.


*** (2) UniverslInterface._trainer() ***

_trainer() is the abstract method defined in UniverslInterface which is where task (2) is to be implemented in subclasses. As it is meant to be written without regard to nimble details, its name is prefaced with an underscore to declare it off limits to a nimble user. Furthermore, the word 'trainer' is employed because it is exactly a function which trains a learner; this is meant to set it (however slightly) apart from UniversalLearner.train() whose wider scope performs (1) and (2), returning a TrainedLearner for the nimble user.

At the UniversalInterface api level we make no distinction between the instantiation of any constructions or objects in the backend package and the training of a learner (if indeed those are separate). Therefore, the the UniversalInterface._trainer() method takes as input the transformed inputs, and must do all of the work in order to produce a trained model. The expectation is that this is a task purely implemented in a derived class; according to whatever limitations are imposed by the backend package.

It is important to note that after training occurs, what is expected by the rest of the code in UniversalInterface will not be a TrainedLearner, and that is not what UniversalInterface._trainer() needs to return. Keeping with the idea that the abstract methods are meant to deal with package level tasks only, the construction of a TrainedLearner object will take place at a higher in the implementation, in UniversalInterface.trainAndApply(), and whatever is returned by _trainer() will never be used outside of the abstract methods. It is suggested that some consistent object (either provided by the API of the underlying package, or constructed by the interface implementor) is used, but this is not enforced by UniversalInterface.

Note that some creativity may be required in order to make different kinds of workflows fit in to what we have described. Imagine, for example, a package which contains a function similar to nimble's trainAndApply() function, which takes both training data and testing data as inputs, and returns only predicted labels as output. Such a function has no intermediate state to be returned in the spirit of UniversalInterface.train(). The suggested solution to such a dilemma is to to have the train() method recognize this case, and return some kind of wrapper which fixes in place the training data. Then, in task (3), the actual call to the backend is made. It is thus the preferred policy of nimble to produce package interfaces which conform with the input and output requirements of the API, even if the actual execution does not match the spirit of the named methods.


*** TrainedLearner ***

The purpose of a TrainedLearner object is that it provides a simple interface for nimble users and the rest of UniversalInterface to interact with whatever package specific backend will be set up by _trainer(). But, the goal is also that it isn't necessary for a new kind of TrainedLearner to be subclassed for each new package. Therefore, all the functionality of a TrainedLearner is actually implemented simply by the abstract methods of UniversalInterface, appropriately wrapped so that they present a nimble appropriate API.

For example: a TrainedLearner offers a retrain() method, which takes a new trainX and trainY, then will build a new Learner using that data. In the backend will, once again, be the UniversalInterface._trainer() method, but it will first be wrapped with code to perform the same kind of argument transformation described in task (1) on the new trainX and trainY inputs (while keeping all the other previously transformed arguments fixed). Since all of the hooks and abstract methods are available in UniversalInterface, UniversalInterface also provides a concrete method (similar to a factory method) called _makeTrainedLearner() which will setup all the appropriate wrappers; the TrainedLearner's __init__ method only assigns the input values to the appropriate attributes. _makeTrainedLearner() is then used directly above in UniversalInterface.train() to construct a the TrainedLearner wrapper object.

Besides for retrain, TrainedLearner is guaranteed to offer a couple of other functions to a nimble user. First is incrementalTrain(), which is how online learners are supported. In UniversalInterface there is an incrementalTrain abstract function which is meant to either extend the current learner's training with the new provided data (which similarly to retrain, has been transformed due to a TrainedLearner wrapper) or will throw an exception if the learner does not support online learning.

Also, there is a method named getAttributes which returns a dict of the attributes of the model discovered by the learner. It too is backed by an abstract method in UniversalInterface and as a pure query with no inputs, it isn't wrapped  in I/O transformation like the other methods of TrainedLearner.

Finally, TrainedLearner will also provide an applyTo() method which gives a nimble user a single function that performs tasks (3) and (4). This is accomplished by wrapping UniversalInterface._applier() with input and output transformation, which is described in the discussion of tasks (3) and (4).

Note though, those are the only functions that are guaranteed, but a package implementor may choose to expose even more. This is discussed further below in *** Other abstract functions *** section.


*** (3) UniversalInterface._applier() ***

UniversalInterface._applier() is the abstract method in which a package implementor is meant to define how a learner interacts with data after it has been trained. In many ways it operates in the same way as UniversalInterface._trainer(). The '_'is used for the same reason, and the name is meant to distinguish it from a TrainedLearner's applyTo() method. Once again, there is a strict distribution of responsibility: UniversalInterface's concrete code (via _makeTrainedLearner()) deals with setting up the input transformation as a wrapper of _applier(), allowing _applier() to deal strictly with the in-package details of whatever task appropriate way that particular Learner interacts with new data (label prediction, transformation, remapping into a different feature space, etc).

Also similarly to _trainer(), the return value of _applier() is not strongly regulated. It is encouraged that whatever the results of the appropriate calls in the package will be passed on to output transformation in order to be massaged into a nimble acceptable type. However, this demonstrates the leeway that the interface implementor has here (and likewise in _trainer()) because there is nothing in the API which stops the implementor from doing the work of I/O transformation within the _applier() method instead of within the transformation methods. If that is easiest, then it is a choice entirely up to the implementor.

Leaving _applier() entirely up to the package implementor again allows for creative implementation of workflows different from what we enforce. For example, consider a package, or single learner, that performs prediction pointwise, not on a matrix of data points crossed by features. A simple solution is to implement batch prediction as a subroutine, resulting in functionality which looks the same from the perspective of both a nimble user and output transformation when compared to other packages.


*** (4) Output transformation ***

Output transformation is implemented much the same way as input transformation. There is a single abstract function through which transformation is defined, named UniversalInterface._outputTransformation(). It takes as input the leaner's name (as context), the value to be transformed, and a copy of the return value of the matching earlier call to _inputTransformation(). Thus, any data about the input transformation that may be needed to perform output transformation will be available in this scope, using the formating constraints described in (1). _outputTransformation() is given total freedom in defining the return value. It is suggested that mirroring the formating of the input values as much as possible would be the most appropriate strategy.

The call to perform output transformation occurs in the TrainedLearner.applyTo() method. The UniversalInterface._applier() only takes the backend learner and a transformed testX as input, but applyTo() is slightly different because it is available to the nimble user. applyTo() takes two parameters: a nimble formated test data set, and a flag named 'scoreMode'. The parameters to _applier() are produced as you would expect, the backend learner is passed from the TrainedLearner object and the test set input to applyTo() is transformed and passed to _applier(). The 'scoreMode' flag is meant to have the same effect as the one in nimble.trainAndApply() and is added at the TrainedLearner level (not the _applier() level) because it deals strictly with the format of the output. The functionality of this flag is accomplished entirely within applyTo() by making use of another abstract method named UniversalInterface.getScores(). In the case of a classifying learner (the only type that is acceptable for non default values of 'scoreMode') getScores() must be implemented to return the scores for each class on each data point, according to their natural ordering. Unfortunately, this is a package level operation for which an output format must be enforced. Other than that exception, all other issues of formating for the two non default values of the flag are dealt with in applyTo().

*** Other abstract functions ***

UniversalInterface also defines other abstract methods to support more than what is necessary to directly implement a call to nimble.trainAndApply(). There are other top level nimble functions that require backend functions, for example nimble.learnerParameters(), but often the backends for those methods are the same as helpers which are required to make trainAndApply() work. There are currently three notable exceptions.

Firstly, elsewhere in nimble it is often necessary to refer to interfaces using short strings, especially when we need to refer to an interface using a function parameter. Therefore, nimble offloads the work of dealing with these small strings to the interfaces themselves. There are two relevant methods: isAlias() takes a string and determines if it refers to this interface, and getCanonicalName() returns a string containing the canonical string for that interface. Both must be be implemented by a derived class in order for it to be able to interface with nimble.

Secondly, one can imagine that there are details of the implementation of an interface which might rely on things specific to the host system, such as the install path, or the location of source files. UniversalInterface offers a group of functions which control how such options are set and accessed, and allow them to be saved in nimble's overall configuration file. The interface implementor determines how many and what the names of any such options are by providing an implementation to the the abstract function UniversalInterface._configurableOptionNames(). This function must return a list of names which will be considered the only valid option names for this interface. Each name is used during instantiation to pull values from nimble's configuration subsystem, which are then stored in a dictionary, available for use in the rest of the instantiated interface. Specifically, UniversalInterface.getOption() and UniversalInterface.setOption() access and modify (respectively) the in-memory values for a given option. By comparison, getDefaultOption() and setDefaultOption() access and modify the values stored in nimble's configuration without changing the currently used values. Furthermore, an interface implementor must specify first time defaults to be used the first time an interface is instantiated on a particular host machine. These must be specified by providing a concrete implementation of UniversalInterface's abstract function _optionDefaults().

Thirdly, note that the implementer of an interface may expose as much of the internal package's functionality as they want, as long as they at least implement the abstract methods defined by UniversalInterface, allowing for top level nimble functions to be used as expected. Two means of exposure are allowed. First, a package implementor may implement whatever methods they may want in their subclass of Universal Interface. These would then be available to a nimble user through the package object. However, any such methods would be limited to things like queries to the package; it would be extremely awkward to force a trained learner's backend to be in the scope of such a function. To manipulate learners, we instead offer a means of exposing functions through the TrainedLearner interface. UniversalInterface's abstract function _exposedFunctions() allows for a package implementor to specify a list of functions they want exposed through every TrainedLearner object produced by that interface. The key insight is that once a TrainedLearner is constructed, the functions in that list will be wrapped with I/O transformation (like TrainedLearner.applyTo()), with the results being assigned as attributes of that TrainedLearner object. Therefore, if those functions are written as they would be methods of a TrainedLearner object (namely taking self as the first argument) then the wrappers will ensure they have access to all a TrainedLearner's attributes, including self.backend, the return value of a call to UniversalInterface._trainer(). Using that, advanced manipulations of learners that a package may offer can then be presented to the nimble user through the TrainedLearner object.

*** Achieving Interaction ***

To be made available through the top level nimble functions (like trainAndApply()), interface implementors need only implement a single subclass of UniversalInterface, and thus will be restricted to modifying a single file. This file must be located in the nimble.interfaces subpackage. Note that nothing else is required in order to integrate the new interface with nimble, this is done automatically. A helper is run from nimble.interfaces.__init__ when nimble is imported, checking all the files in nimble.interfaces subpackage for modules that contain the class definition of a subclass of UniversalInterface. The helper then attempts to instantiate each of these classes that it has found, and the results of all successful instantiations are added to a list called nimble.interfaces.available. Thus, top level nimble functions (and a nimble user) have access to a list of active interfaces. There are a couple of consequences to this design. Firstly, a package's __init__ method must take no arguments; if there is anything that determines how the interface operates, it is to be defined using configurable options. Secondly, an interface is only meant to be instantiated if it is currently working on that host machine. As mentioned in the introduction, object instantiation is the ideal place to do setup, warmup, and validation of the underlying package. The expected failure state, if the package is not usable, is to fail during instantiation. Thrown exceptions will be caught and registered with nimble's sub system for logging warnings. Users may then review the failed instantiations at their leisure.



